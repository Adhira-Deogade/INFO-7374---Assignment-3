{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Expt2-BoW-imdb-mlm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adhira-Deogade/INFO-7374---Assignment-3/blob/master/Expt2_BoW_imdb_mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9RdgzlbCH-B2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abe9b85f-d8ff-42e5-df20-3182d5ae9bb0"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from numpy import array\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rIltfg9MIFbO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZvN8VsiTIO33",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K5Mn_-3TISQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gHeqcQtsIVHx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PPQnpYh_IdXj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluate a neural network model\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "\tscores = list()\n",
        "\tn_repeats = 30\n",
        "\tn_words = Xtest.shape[1]\n",
        "\tfor i in range(n_repeats):\n",
        "\t\t# define network\n",
        "\t\tmodel = Sequential()\n",
        "\t\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "\t\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t\t# compile network\n",
        "\t\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t\t# fit network\n",
        "\t\tmodel.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
        "\t\t# evaluate\n",
        "\t\tloss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "\t\tscores.append(acc)\n",
        "\t\tprint('%d accuracy: %s' % ((i+1), acc))\n",
        "\treturn scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "93aRROn7IfPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare bag of words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "\t# create the tokenizer\n",
        "\ttokenizer = Tokenizer()\n",
        "\t# fit the tokenizer on the documents\n",
        "\ttokenizer.fit_on_texts(train_docs)\n",
        "\t# encode training data set\n",
        "\tXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "\t# encode training data set\n",
        "\tXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WKgmVWMcIhZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}